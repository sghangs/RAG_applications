{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi model RAG with Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements one of the multiple ways of multi-model RAG. It extracts and processes text and images from PDFs, utilizing a multi-modal Retrieval-Augmented Generation (RAG) system for summarizing and retrieving content for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"data/Attention.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=[]\n",
    "image_data=[]\n",
    "\n",
    "with pymupdf.open(file_path) as pdf_file:\n",
    "    #Create directory to store the images\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "    \n",
    "    #loop through every page in pdf\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]\n",
    "\n",
    "        #get the text on page\n",
    "        text = page.get_text().strip()\n",
    "        text_data.append({'response':text,\"name\":page_number+1})\n",
    "\n",
    "        #Get the list of images on teh page\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        #loop through all images on teh page\n",
    "        for image_index,img in enumerate(images):\n",
    "            xref=img[0]\n",
    "            base_image = pdf_file.extract_image(xref) #get base image\n",
    "            image_bytes = base_image[\"image\"]  #get images bytes\n",
    "            image_ext = base_image[\"ext\"] #get image extension\n",
    "\n",
    "            #Load the image using PIL and save it\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyBKkv7DTUdkj9oGgm6vwvdgS7Hqb0BN7Qs')\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"extracted_images\"):\n",
    "    image = Image.open(f\"extracted_images/{img}\")\n",
    "    response = model.generate_content([image, \"You are an assistant tasked with summarizing tables, images and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text or image:\"])\n",
    "    image_data.append({\"response\": response.text, \"name\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'This image depicts the architecture of a Transformer decoder.  It shows the flow of information from inputs through embedding, positional encoding, masked and unmasked multi-head attention layers, feed forward networks, add & norm layers, and finally to output probabilities via a linear and softmax layer. The decoder processes sequences sequentially, indicated by the \"shifted right\" outputs.  Key components include multi-head attention for context understanding and feed forward networks for transformation.\\n',\n",
       "  'name': 'image_3_1.png'},\n",
       " {'response': \"This image shows a diagram of a scaled dot-product attention mechanism.  The process starts with three inputs (V, K, Q), which are each passed through linear layers. The outputs of these layers are fed into a scaled dot-product attention module. The attention module's output is concatenated and then passed through another linear layer to produce the final output (h).\\n\",\n",
       "  'name': 'image_4_1.png'},\n",
       " {'response': 'This image shows a diagram of the steps in a scaled dot-product attention mechanism.  The steps include matrix multiplication (MatMul), scaling, optional masking, softmax, and a final matrix multiplication.  The inputs are Q, K, and V.\\n',\n",
       "  'name': 'image_4_2.png'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#Load the document\n",
    "docs_list = [Document(page_content=text['response'],metadata={'name':text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'],metadata={'name':img['name']}) for img in image_data]\n",
    "\n",
    "#Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs_list)\n",
    "img_split = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs_split + img_split,\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriver = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k':1} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "query = \"What is the BLEU score of the Transformer (base model)?\"\n",
    "docs = retriver.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'name': 8}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 路 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 路 1019\\n1.4 路 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 路 1018')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the BLEU score for the Transformer model is not explicitly mentioned in the given table. However, it is stated that the Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":docs[0].page_content, \"question\": query})\n",
    "print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
