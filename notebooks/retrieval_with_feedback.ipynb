{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG System with Feedback Loop: Enhancing Retrieval and Response Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system implements a Retrieval-Augmented Generation (RAG) approach with an integrated feedback loop. It aims to improve the quality and relevance of responses over time by incorporating user feedback and dynamically adjusting the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from utility import encode_pdf, show_context, retrieve_context_per_question\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Any, Dict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import faiss\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from utility import replace_t_with_space\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import pymupdf\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_to_string(path):\n",
    "    \"\"\"\n",
    "    Read a PDF document from the specified path and return its content as a string.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the PDF document.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated text content of all pages in the PDF document.\n",
    "\n",
    "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
    "    extract the text content from each page, and append it to a single string.\n",
    "    \"\"\"\n",
    "    # Open the PDF document located at the specified path\n",
    "    doc = pymupdf.open(path)\n",
    "    content = \"\"\n",
    "    # Iterate over each page in the document\n",
    "    for page_num in range(len(doc)):\n",
    "        # Get the current page\n",
    "        page = doc[page_num]\n",
    "        # Extract the text content from the current page and append it to the content string\n",
    "        content += page.get_text()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_from_string(content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a string into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        content (str): The text content to be encoded.\n",
    "        chunk_size (int): The size of each chunk of text.\n",
    "        chunk_overlap (int): The overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A vector store containing the encoded content.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input content is not valid.\n",
    "        RuntimeError: If there is an error during the encoding process.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(content, str) or not content.strip():\n",
    "        raise ValueError(\"Content must be a non-empty string.\")\n",
    "\n",
    "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be a positive integer.\")\n",
    "\n",
    "    if not isinstance(chunk_overlap, int) or chunk_overlap < 0:\n",
    "        raise ValueError(\"chunk_overlap must be a non-negative integer.\")\n",
    "\n",
    "    try:\n",
    "        # Split the content into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "\n",
    "        # Assign metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata['relevance_score'] = 1.0\n",
    "\n",
    "        # Generate embeddings and create the vector store\n",
    "        #Embeddings \n",
    "        embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred during the encoding process: {str(e)}\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "#create vector store\n",
    "content = read_pdf_to_string(file_path)\n",
    "vector_store = encode_from_string(content)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "system_prompt = \"\"\" \n",
    "    Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm,prompt)\n",
    "retriever_chain = create_retrieval_chain(retriever,qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to format user feedback\n",
    "def get_user_feedback(query,response,relevance,quality,comments=\"\"):\n",
    "    return {\n",
    "        \"query\":query,\n",
    "        \"response\":response,\n",
    "        \"relevance\":int(relevance),\n",
    "        \"quality\":int(quality),\n",
    "        \"comments\":comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to store the feedback in json format\n",
    "import json\n",
    "def store_feedback(feedback):\n",
    "    with open(\"data/feedback_data.json\",\"a\") as file:\n",
    "        json.dump(feedback,file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the feedback file\n",
    "def load_feedback_data():\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(\"data/feedback_data.json\",\"r\") as file:\n",
    "            for line in file:\n",
    "                feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback file found. Starting with empty feedback\")\n",
    "    return feedback_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to adjust relevance score in the documents\n",
    "\n",
    "#SChema for llm output\n",
    "class Response(BaseModel):\n",
    "    answer : str = Field(description=\"Then answer to the question. The options can be 'Yes' or 'No'\")\n",
    "\n",
    "def adjust_relevance_score(query:str,docs:List[Any],feedback_data:List[Dict[str,Any]]) -> List[Any]:\n",
    "\n",
    "    #prompt for relevance check\n",
    "    prompt = PromptTemplate(\n",
    "        template = \"\"\"Determine if the following feedback response is relevant to the current query and document content.\n",
    "        You are also provided with the Feedback original query that was used to generate the feedback response.\n",
    "        Current query: {query}\n",
    "        Feedback query: {feedback_query}\n",
    "        Document content: {doc_content}\n",
    "        Feedback response: {feedback_response}\n",
    "        \n",
    "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
    "        \"\"\"\n",
    "    )\n",
    "    relevance_chain = prompt | llm.with_structured_output(Response)\n",
    "\n",
    "    #Loop each documents retreived for current query\n",
    "    for doc in docs:\n",
    "        relevant_feedback = []\n",
    "        # loop each feedback data to check the relevance with document and current query\n",
    "        for feedback in feedback_data:\n",
    "            input = {\n",
    "                \"query\":query,\n",
    "                \"feedback_query\":feedback[\"query\"],\n",
    "                \"doc_content\":doc.page_content,\n",
    "                \"feedback_response\":feedback[\"response\"]\n",
    "            }\n",
    "\n",
    "            result = relevance_chain.invoke(input).answer\n",
    "\n",
    "            if result == \"yes\":\n",
    "                relevant_feedback.append(feedback)\n",
    "\n",
    "        #Adjust the relevance score of the document based on feedback\n",
    "        if relevant_feedback:\n",
    "            average_relevance = sum(f[\"relevance\"] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            doc.metadata['relevance_score'] *= (average_relevance / 3) # Assuming 1-5 scale, 3 is neutral\n",
    "    \n",
    "    #Rerank documents based on relevance score\n",
    "    return sorted(docs,key=lambda x:x.metadata[\"relevance_score\"],reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstration of how to retrieve answers with respect to user feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunny\\AppData\\Local\\Temp\\ipykernel_22656\\2427385771.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "\n",
    "# Get response from RAG system\n",
    "response = retriever_chain.invoke({\"input\":query})[\"answer\"]\n",
    "\n",
    "relevance = 5\n",
    "quality = 4\n",
    "\n",
    "# Collect feedback\n",
    "feedback = get_user_feedback(query, response, relevance, quality)\n",
    "\n",
    "# Store feedback\n",
    "store_feedback(feedback)\n",
    "\n",
    "# Adjust relevance scores for future retrievals\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "adjusted_docs = adjust_relevance_score(query, docs, load_feedback_data())\n",
    "\n",
    "# Update the retriever with adjusted docs\n",
    "retriever.search_kwargs['k'] = len(adjusted_docs)\n",
    "retriever.search_kwargs['docs'] = adjusted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fine tune the vector index to include query and their resonse which are having good feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n",
    "    # Filter high-quality responses\n",
    "    good_responses = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    # Extract queries and responses, and create new documents\n",
    "    additional_texts = []\n",
    "    for f in good_responses:\n",
    "        combined_text = f['query'] + \" \" + f['response']\n",
    "        additional_texts.append(combined_text)\n",
    "\n",
    "    # make the list a string\n",
    "    additional_texts = \" \".join(additional_texts)\n",
    "    \n",
    "    # Create a new index with original and high-quality texts\n",
    "    all_texts = texts + additional_texts\n",
    "    new_vectorstore = encode_from_string(all_texts)\n",
    "    \n",
    "    return new_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune the vectorstore periodicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periodically (e.g., daily or weekly), fine-tune the index\n",
    "new_vectorstore = fine_tune_index(load_feedback_data(), content)\n",
    "retriever = new_vectorstore.as_retriever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
