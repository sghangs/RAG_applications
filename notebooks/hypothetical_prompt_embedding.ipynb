{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothetical Prompt Embeddings (HyPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overview\n",
    "This code implements a Retrieval-Augmented Generation (RAG) system enhanced by Hypothetical Prompt Embeddings (HyPE). Unlike traditional RAG pipelines that struggle with query-document style mismatch, HyPE precomputes hypothetical questions during the indexing phase. This transforms retrieval into a question-question matching problem, eliminating the need for expensive runtime query expansion techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from utility import encode_pdf, show_context, retrieve_context_per_question\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import faiss\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from utility import replace_t_with_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\"\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyPERetriever:\n",
    "    def __init__(self,file_path,chunk_size=1000,chunk_overlap=200):\n",
    "        self.file_path = file_path\n",
    "        self.llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        self.question_gen_prompt = PromptTemplate(\n",
    "            template = \"\"\"Analyze the input text and generate essential questions that, when answered, \n",
    "        capture the main points of the text. Each question should be one line, \n",
    "        without numbering or prefixes.\\n\\n \n",
    "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\"\",\n",
    "            input_variables=['chunk_text']\n",
    "        )\n",
    "\n",
    "        self.question_chain = self.question_gen_prompt | self.llm | StrOutputParser()\n",
    "    \n",
    "    def generate_questions(self,chunk_text):\n",
    "        \"\"\"\n",
    "        Uses the LLM to generate multiple hypothetical questions for a single chunk.\n",
    "        These questions will be used as 'proxies' for the chunk during retrieval.\n",
    "\n",
    "        Parameters:\n",
    "        chunk_text (str): Text contents of the chunk\n",
    "\n",
    "        Returns:\n",
    "        chunk_text (str): Text contents of the chunk. This is done to make the \n",
    "            multithreading easier\n",
    "        hypothetical prompt embeddings (List[float]): A list of embedding vectors\n",
    "            generated from the questions\n",
    "        \"\"\"\n",
    "        response = self.question_chain.invoke({'chunk_text':chunk_text}).replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
    "        return chunk_text,self.embeddings.embed_documents(response)\n",
    "    \n",
    "    def create_vectorstore(self,chunks : List[str]):\n",
    "        \"\"\"\n",
    "        Creates and populates a FAISS vector store from a list of text chunks.\n",
    "\n",
    "        This function processes a list of text chunks in parallel, generating \n",
    "        hypothetical prompt embeddings for each chunk.\n",
    "        The embeddings are stored in a FAISS index for efficient similarity search.\n",
    "\n",
    "        Parameters:\n",
    "        chunks (List[str]): A list of text chunks to be embedded and stored.\n",
    "\n",
    "        Returns:\n",
    "        FAISS: A FAISS vector store containing the embedded text chunks.\n",
    "        \"\"\"\n",
    "        vector_store = None\n",
    "\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            #Use threading to speed up the generation of prompt embeddings\n",
    "            futures = [pool.submit(self.generate_questions,c) for c in chunks]\n",
    "            #process embeddings as they complete\n",
    "            for f in tqdm(as_completed(futures), total=(len(chunks)/3)):\n",
    "                chunk,vectors = f.result()  #Retrieved the processed chunk and its embeddings\n",
    "\n",
    "                #initialize the faiss vectorstore on first chunk\n",
    "                if vector_store == None:\n",
    "                    vector_store = FAISS(\n",
    "                        embedding_function=self.embeddings,  #define embedding model\n",
    "                        index = faiss.IndexFlatL2(len(vectors[0])),  #define L2 index for similarity search\n",
    "                        docstore = InMemoryDocstore(),  # Use in memory document storage\n",
    "                        index_to_docstore_id={}   # maintain index to document mapping\n",
    "                    )\n",
    "                \n",
    "                # Pair the chunk's content with each generated embedding vector.\n",
    "                # Each chunk is inserted multiple times, once for each prompt vector\n",
    "                chunks_with_embedding_vector = [(chunk.page_content,vec) for vec in vectors] \n",
    "\n",
    "                #Add embeddings to the vector store\n",
    "                vector_store.add_embeddings(chunks_with_embedding_vector)\n",
    "        return vector_store\n",
    "\n",
    "    def encode_pdf(self,path):\n",
    "        \"\"\"\n",
    "        Encodes a PDF book into a vector store using HuggingFace embeddings.\n",
    "\n",
    "        Args:\n",
    "            path: The path to the PDF file.\n",
    "            chunk_size: The desired size of each text chunk.\n",
    "            chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "        Returns:\n",
    "            A FAISS vector store containing the encoded book content.\n",
    "        \"\"\"\n",
    "        #Load the Pdf file \n",
    "        loader=PyPDFLoader(path)\n",
    "        docs=loader.load()\n",
    "\n",
    "        #Split the documents into chunks\n",
    "        splitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "        texts=splitter.split_documents(docs)\n",
    "        cleaned_texts=replace_t_with_space(texts[0:20])\n",
    "\n",
    "        #Create vector store\n",
    "        vectorstore=self.create_vectorstore(cleaned_texts)\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    def retriever(self,vectorstore):\n",
    "        chunk_query_retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "        return chunk_query_retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret_object = HyPERetriever(path)\n",
    "#vector_store = ret_object.encode_pdf(path)\n",
    "#ret = ret_object.retriever(vector_store)\n",
    "#test_query = \"What is the main cause of climate change?\"\n",
    "#context = retrieve_context_per_question(test_query,ret)\n",
    "#show_context(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ab9b366ad6c6c880fffbd1a9e1c0ba9825d2f9bd67635de77c6498f87926fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
