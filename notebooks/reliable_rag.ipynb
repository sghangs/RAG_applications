{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information.\n",
    "Check for retrieved document relevancy and highlight the segment of docs used for answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
    "]\n",
    "\n",
    "loader=WebBaseLoader(web_paths=urls)\n",
    "\n",
    "docs=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Splitting\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "docs_split=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\Desktop\\Langchain_projects\\Q&A_chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Embeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#Vectorstore\n",
    "vector_store=Chroma.from_documents(\n",
    "    documents=docs_split,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"my_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever\n",
    "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the differnt kind of agentic design patterns?\"\n",
    "retrieved_docs=retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies.\\xa0Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check document relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "#Output Schema\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" \n",
    "    Binary score of the retrieved documents for relevance check\n",
    "    \"\"\"\n",
    "    binary_score:str = Field(description=\"Document relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\" \n",
    "You are a grader assessing relevance of retrieved documents for the user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"user\",\"Retrieved documents : {documents} \\n User question : {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "grade_chain = grade_prompt | structured_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies. Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Agentic Design Patterns Part 2: Reflection✨ New course! Enroll in Building AI Voice Agents for ProductionExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 2, Reflection Large language models can become more effective agents by reflecting on their own behavior.LettersTechnical InsightsPublishedMar 27, 2024Reading time2 min readShareDear friends,Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool Use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='no' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_to_use = []\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content, '\\n', '-'*50)\n",
    "    score = grade_chain.invoke({'documents':doc.page_content,'question':question})\n",
    "    print(score,'\\n')\n",
    "    if score.binary_score == 'yes':\n",
    "        docs_to_use.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt=PromptTemplate(\n",
    "    template=\"\"\" \n",
    "    You are assistant for question answering tasks.\n",
    "    Use the following piece of retreived context to answer\n",
    "    the question.If you don't know the answer, say that you don't know.\n",
    "    keep the answer concise.\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    \"\"\",\n",
    "    input_variables=['context','question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    context_text=\"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text\n",
    "\n",
    "parllel_chain=RunnableParallel({\n",
    "    'context':retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "rag_chain = parllel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, there are four agentic design patterns mentioned:\\n\\n1. Reflection\\n2. Tool Use\\n3. Planning\\n4. Multi-Agent Collaboration'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "#Output Schema\n",
    "class GradeHallucination(BaseModel):\n",
    "    \"\"\" \n",
    "    Binary score of the hallucination present in answer\n",
    "    \"\"\"\n",
    "    binary_score:str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(GradeHallucination)\n",
    "\n",
    "system_prompt = \"\"\" \n",
    "You are a grader assessing whether llm generation answer is grounded in / supported by set of retrieved facts.\n",
    "Give a binary score 'yes' or 'no'. 'yes' means answer is grounded in / supported by set of retrieved facts.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"user\",\"Retrieved facts : {documents} \\n llm generation : {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_chain = hallucination_prompt | structured_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "hallucination_score = hallucination_chain.invoke(\n",
    "    {'documents':format_docs(retrieved_docs),'generation':answer})\n",
    "print(hallucination_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ab9b366ad6c6c880fffbd1a9e1c0ba9825d2f9bd67635de77c6498f87926fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
